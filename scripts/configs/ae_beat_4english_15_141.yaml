is_train: True 
out_root_path: /outputs/audio2pose/ 
train_data_path: /datasets/beat_cache/beat_4english_15_141/train/ 
val_data_path: /datasets/beat_cache/beat_4english_15_141/val/ 
test_data_path: /datasets/beat_cache/beat_4english_15_141/test/ 
mean_pose_path: /datasets/beat_cache/beat_4english_15_141/train/
std_pose_path: /datasets/beat_cache/beat_4english_15_141/train/
torch_hub_path: /datasets/checkpoints/ 
dataset: beat

new_cache: True
cache_path: ae_topology_8

pose_rep: bvh_rot_6d #bvh_rot_6d #bvh_axis_angle
pose_file_extension: 6d #6d #axisangle
head_rep: head_rot
audio_rep: wave16k
facial_rep: facial52
word_rep: text
emo_rep: emo
sem_rep: sem
speaker_id: False
audio_norm: True
facial_norm: True
#pose_dims: 141
pose_length: 32
output_joint_dims: 3 #euler angles
train_joint_dims: 6 #3 (axis-angle) #6 (Rot6d)
eval_joint_dims: 6 #3 #6
batch_size: 256
test_period: 20 #1 #20
stride: 10
vae_length: 216 #300
hidden_size: 128
g_name: VAESKConv #VAESKConv #VAEConvMLP #EmbeddingNet
model: motion_autoencoder
trainer: ae
variational_encoding: False
rec_weight: 1 #1
vel_weight: 0.1 #0.1
kld_weight: 0
lr_base: 0.00012 
grad_norm: 0
epochs: 400
test_period: 20 #1 #20